\subsubsection{Objective Function}
The ability to learn (i.e. train) an ANN model to fit for a specific context has been the foundation for the recent resurgence of AI. This learning ability can be represented as an optimization problem, where the optimum network model $f^*$ can be derived from optimising (i.e. minimising) a cost function $\mathcal{C}$ such that $C(f) \geq C(f^*)$, given that the cost of an optimum model is the global minimum. Once the cost function defined, the learning process or model training phase is then simply the process of finding such a model $f$ that is $f^*$ or close to be $f^*$. The cost function often referred as loss function, i objective function in an optimisation problem. is used interchangablly 
\\
An optimization objective function in machine learning is used to measure the difference between the predicted outcome and expected outcome, which can be optimized during training phase to increase the accuracy of a prediction. A typical objective function often composes of the regularization term and loss term. The collective efforts to generalize the prediction error is summarised as regularization term~\cite{goodfellow_2015}, the loss term describes the measured off-target discrepancy between prediction using model $f(x|\phi)$ (where $x$ is the input vector and $\phi$ is model parameter matrix) and the actual target. A typical optimization objective function can have the form as:

\begin{equation}
    \mathcal{Obj}(,y,\phi) = 
\end{equation}