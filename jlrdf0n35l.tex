\subsubsection{Objective Function}
The ability to learn (i.e. train) an ANN model to fit for a specific context has been the foundation for the recent resurgence of AI. This learning ability can be represented as an optimization problem, where the optimum network model $f^*$ can be derived from optimising (i.e. minimising) a cost function $\mathcal{C}$ such that $C(f) \geq C(f^*)$, given that the cost of an optimum model is the global minimum. Once the cost function defined, the learning process or model training phase is then simply the process to find such a model $f$ that is $f^*$ or close to be $f^*$. The cost function often referred as loss function in mathematics, econometrics or computational neuroscience, in an optimisation problem, objective function in commonly used interchangeably with minute difference, where object function can take be both $\pm~C$ and the objective for training is the exercise of minimising or maximising the optimization objective function by changing model parameters $\phi$. In this
\\
The purpose of an optimization objective function in machine learning is used to measure the difference between the predicted outcome and expected outcome, which can be optimized during training phase to increase the accuracy of a prediction. A typical objective function often composes of the regularization term and loss term. The collective efforts to generalize the prediction error is summarised as regularization term~\cite{goodfellow_2015}, the loss term describes the measured off-target discrepancy between prediction using model $f(x|\phi)$ (where $x$ is the input vector and $\phi$ is model parameter matrix) and the actual target. A typical optimization objective function can have the form as:

\begin{equation}
    \mathcal{Obj}(,y,\phi) = 
\end{equation}