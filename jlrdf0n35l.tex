\subsubsection{Objective Function}
The ability to learn (i.e. train) an ANN model to fit for a specific context has been the foundation for the recent resurgence of AI. This learning ability can be represented as an optimization problem, where the optimum network model $f^*$ can be derived from optimising (i.e. minimising) a cost function $\mathcal{C}$ such that $C(f) \geq C(f^*)$, given that the cost of an optimum model is the global minimum. Once the cost function defined, the learning process or model training phase is then simply the process to find such a model $f$ that is $f^*$ or close to be $f^*$. The cost function often referred as loss function in mathematics, econometrics or computational neuroscience; in an optimisation problem, objective function is commonly used interchangeably with minute difference, where object function take both $\pm~C$ and the objective for training is the exercise of minimising or maximising the optimization objective function by changing model parameters $\phi$. In this article, we will use cost function and optimization objective function exchangeably in the context of machine learning.
\\
The implementation of an optimization objective function in machine learning applies the same principle and generally adopts two terms regularization and loss term. Regularization term has the main purpose to reduce the complexity of model parameters and reduce concerns over overfitting. A typical optimization objective function can have the form as:

\begin{equation}
    \mathcal{Obj}(\mathcal{X},y,\phi) = \mathcal{L}(f(\mathcal{X}|\phi) + \lambda \mathcal{R} (\phi) 
\end{equation}

where $\lamda$ is the regularization parameter, which can be trained for individual model. $\mathcal{L}(f(\mathcal{X}|\phi)$ measures the difference between the predicted outcome and expected outcome, which often adopts cross entropy form and can be optimized during training. A typical objective function used in ANN needs to be differentiable and this is because in training modern ANN we often use back-propagation method~\cite{LeCun_2015,Heaton_2017} to update the weights of each neuron in the network, which involves calculating the gradient of the objective function with respect to weights $\frac {\partial{C}} {\partial{w_{ij}}} $. Objective function used for this purpose often compose of two terms: 