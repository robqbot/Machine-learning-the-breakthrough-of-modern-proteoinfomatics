\subsubsection{Gradient Calculation}
Backpropagation algorithm (often referred as backprop) was proposed by~\citet{werbos1975beyond}, which provided the fundamentals for calculating the gradient of the objective function $\mathcal{Obj}$ efficiently. Hence make training multi-layers networks a computationally viable. 
\par
Calculating gradient of the objective function $\mathcal{Obj}$ numeris numerical evaluation is computational expensive, backprop offers an economical solution. The computing derivatives is by propagating information through a general network 

manner by propagating error term back through layers via modifying the weight of each node. 
\par 
Back propagation calculations often needs to summarise many tensors. To calculate each tensor seperately involve large expense of memory, in practise, often adopt a single buffer approach where each value is only added to the buffer after computation.