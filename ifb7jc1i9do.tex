\subsubsection{Gradient Calculation}
Backpropagation algorithm (often referred as backprop) was proposed by~\citet{werbos1975beyond}, which provided the fundamentals for calculating the gradient of the objective function $\mathcal{Obj}$ efficiently. Hence make training multi-layers networks a computationally viable. 
\par
Calculating gradient of the objective function $\mathcal{Obj}$ numerically is computational expensive, backprop computes derivatives by propagating error term back through network via modifying the weight of each node, which is in general computational efficient comparing to matrix manipulations. 
\par
The gradient of a scaler $\mathcal{z}$ with respect to $\mathcal{x}$ can be derived from the start gradient multiply by the Jacobian that produce $\mathcal{z}$ until reach starting $\mathcal{x}$, for divergingnode

Back propagation calculations often needs to summarise many tensors. To calculate each tensor seperately involve large expense of memory, in practise, often adopt a single buffer approach where each value is only added to the buffer after computation.