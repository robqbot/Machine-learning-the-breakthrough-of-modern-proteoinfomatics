\subsubsection{Gradient Calculation}
Backpropagation algorithm (often referred as backprop) was proposed by~\citet{werbos1975beyond}, which provided the fundamentals for calculating the gradient of the objective function $\mathcal{Obj}$ efficiently. Hence make training multi-layers networks a computationally viable. 
\par
Calculating gradient of the objective function $\mathcal{Obj}$ numerically is computational expensive, backprop computes derivatives by propagating error term back through network via modifying the weight of each node, which is in general computational efficient comparing to matrix manipulations. 
\par
The gradient of a scaler $\mathcal{z}$ with respect to $\mathcal{x}$ can be derived from the start gradient multiply by the Jacobian that produce $\mathcal{z}$ until reach starting $\mathcal{x}$, for nodes with diverging branches, the sum of gradients from each branch will be taken. Nodes in the neural networks can be treated as a variable and commonly described by tensor $\mathcal{V}$. Jacobian of $\mathcal{z}$ with respect to tensor $\mathcal{V}$ in a single layer network construction, where $Y=\mathcal{g}(\mathcal{V}$ and $\mathcal{z} = $can then be written as:

\begin{equation}
    \nabla_{\mathcal{V}}\mathcal{z} = \sum_i (\nabla_{\mathcal{\mathcal{V}}} Y_i)\frac{\partial\mathcal{z}}{\partial Y_i} = \sum_j \sum_i \frac{\partial Y_i} {\partial{\mathcal{V}}_j}\frac{\partial\mathcal{z}}{\partial Y_i}
\end{equation}

Back propagation calculations often needs to summarise many tensors. To calculate each tensor seperately involve large expense of memory, in practise, often adopt a single buffer approach where each value is only added to the buffer after computation.