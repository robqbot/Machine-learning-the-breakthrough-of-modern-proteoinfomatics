\subsubsection{Gradient Calculation}
Backpropagation algorithm (often referred as backprop) was proposed by~\citet{werbos1975beyond}, which provided the fundamentals for calculating the gradient of the objective function $\mathcal{Obj}$ efficiently. training multi-layers networks in a computational efficient manner by propagating error term back through layers via modifying the weight of each node. 
\par
One challenge in calculating gradient of the objective function $\mathcal{Obj}$ is numerical evaluation is computational expensive, backprop offers an economical solution. The computing derivatives is by propagating information through a general network 
\par 
Back propagation calculations often needs to summarise many tensors. To calculate each tensor seperately involve large expense of memory, in practise, often adopt a single buffer approach where each value is only added to the buffer after computation.