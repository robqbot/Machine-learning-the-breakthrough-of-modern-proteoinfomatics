\subsubsection{Training Algorithms}
Backpropagation algorithm (often referred as backprop) was proposed by~\citet{werbos1975beyond}, which provided the fundamentals for training multi-layers networks in a computational efficient manner by propagating error term back through layers via modifying the weight of each node. 
\par
One challenge in calculating gradient of the objective function $\mathcal{Obj$ is numerical evaluation is computational expensive, backprop offers an economical solution. The computing derivatives is propagating information 
\par 
Back propagation calculations often needs to summarise many tensors. To calculate each tensor seperately involve large expense of memory, in practise, often adopt a single buffer approach where each value is only added to the buffer after computation.