\subsubsection{Challenges in Model Training}

Optimization is traditionally very difficult task. Model training in machine learning avoids many difficulties by carefully construct objective function to be continuous, differentiable and convex. However, there are still a few common problems need to be discussed. 

\begin{enumerate}
    \item Ill conditioning\\
Ill conditioning is one of the most common challenges in numerical optimization. The condition number is a measurement of how sensitive the output of a model is to a small change in input. Commonly the condition number is used to measure the tolerance of a model to an input error.  In multi-dimensional space, it is common to have different curvature (defined by $2^\circ$) at different direction

Hessian matrix $\mathrm{H}$ is defined as Jacobian of the gradient, which represents curvature of the $\mathcal{Obj}(\phi)$ and takes form as 
\begin{equation}
    \mathrm{H}(f)(x)_{i,j} = \frac{\partial^2}{\partial x_i \partial x_j}f(x)
\end{equation}

    \item Saddle Points and Flat Regions\\
In high dimensional model trainings, saddle points are more commonly encounter than local extremes (\citet{DauphinPGCGB14} provided a detailed review on theory), where it is both a local minimum within one cross-section and local maximum at a different cross section (Figure~\ref{555041}). At saddle points, Hessian matrix is filled with both positive and negative eigenvalues, where saddle points sit on the direction with positive eigenvalues have local minimum while on the direction with negative eigenvalues have local maximum. \\
Proliferation of saddle points in high dimensional model trainings is problematic for Newton's method, where the algorithm is more prone to the attraction of saddle points. Although~\citet{DauphinPGCGB14} has invented a modified version of Newton's method this is claimed to be saddle-free, a larger application of such second order approach remains difficult. At the same time, first order algorithms like gradient descent seems to be able to escape such saddle regions~\cite{GoodfellowV14}.\\
In addition to saddle points, flat regions of the optimisation space with constant value also posses challenges in model training. In those regions, both gradient and Hessian are zero, which in turn all numerical evaluation mechanisms based on first order and second order can not function, however, in a convex setting, as almost all model trainings in machine learning are, such region corresponds to global minima. 
    \item Poor Local Representation \\
Poor local representation can trick optimisation gradient to the wrong direction, especially when the direction of local minimum contradicts to global minimum. This can happen during exploring $\mathcal{Obj}(\phi)$ space for optimum solution and oppose challenges for model training. \\
Much of the efforts in research have been invested in finding the trouble-free optimisation start position that leads to convergence towards global optimum and avoids complex global structure. On the other hand, minimum success has achieved in developing algorithms that can escape the trap of local features. Almost all Gradient descent and 
\end{enumerate}

