\subsubsection{Challenges in Model Training}

Optimization is traditionally very difficult task. Model training in machine learning avoids many difficulties by carefully construct objective function to be continuous, differentiable and convex. However, there are still a few common problems need to be discussed. 

\begin{enumerate}
    \item Ill conditioning\\
Ill conditioning is one of the most common challenges in numerical optimization. The condition number is a measurement of how sensitive the output of a model is to a small change in input. Commonly the condition number is used to measure the tolerance of a model to an input noise. In machine learning, this challenge is most prevalent in second-order algorithm based model training.\\

Given the model step evaluation as:

\begin{equation}
    f(x_0-\mu\mathcal{g}) \sim f(x_0) - \mu g^Tg + \frac{1}{2}\mu^2g^T\mathrm{H}g
\end{equation}
where $\mu$ is model training rate, $x_0$ is the previous evaluation point, $g$ and $\mathrm{H}$ are the gradient and Hessian matrix at $x_0$ respectively, therefore, current evaluation point given by $x_0-\mu\mathcal{g}$. The term $\mu g^Tg$ defines the improvement quantity, $\frac{1}{2}\mu^2g^T\mathrm{H}g$ defines the correction due to curvature. Ill conditioning occurs when curvature correction term is overpowering the improvement quantity determined by gradient, which essentially means an impaired model training performance. \\
\citet{Goodfellow-et-al-2016} argue although some methods like Newton's method can resolve the conflicts imposed by Hessian matrix $\mathrm{H}$ poor conditioning, but the application of Newton's method in training neural network models requires heavy modification and is inefficient and error prone. 
    \item Local Minima \\
Local minima can be miss-leading particular the exploring territory is extremely rugged (Figure~\ref{555041}), gradient based training algorithms are prone to stuck at local extremes. Although in neural networks trainings, local minima are not necessarily disastrous as long as those local minima are not raising too much cost for $Obj(\phi)$, which we are trying to minimise. In machine learning, model trainings are largely an empirical based practise, due to non-identifia 
    \item Saddle Points and Flat Regions\\
In high dimensional model trainings, it is common to have different curvature (defined by $2^\circ$) at different directions. In fact, saddle points are more commonly encountered than local extremes (\citet{DauphinPGCGB14} provided a detailed review on theory), where they are both a local minimum within one cross-section and a local maximum within a different cross section (Figure~\ref{555041}). At saddle points, Hessian matrix is filled with both positive and negative eigenvalues, where saddle points sit on the direction with positive eigenvalues have local minimum while on the direction with negative eigenvalues have local maximum. In multi-dimensional space,  \\
Proliferation of saddle points in high dimensional model trainings is problematic for Newton's method, where the algorithm is more prone to the attraction of saddle points. Although~\citet{DauphinPGCGB14} has invented a modified version of Newton's method this is claimed to be saddle-free, a larger application of such second order approach remains difficult. At the same time, first order algorithms like gradient descent seems to be able to escape such saddle regions~\cite{GoodfellowV14}.\\
In addition to saddle points, flat regions of the optimisation space with constant value also posses challenges in model training. In those regions, both gradient and Hessian are zero, which in turn all numerical evaluation mechanisms based on first order and second order can not function, however, in a convex setting, as almost all model trainings in machine learning are, such region corresponds to global minima. 
    \item Cliffs \\
It is possible to encounter regions with steep gradient variations, this is particular the issue when evaluating deep neural networks, where multiplying several big weight nodes across many layers is common. As the optimisation gradient approaches this cliff region, the model update step can move extremely violently and miss the optimum solution all together. \\
An effective measure when expecting the presence of cliffs within model training space, is gradient clipping heuristic algorithm, which improves traditional gradient method by taking a modified step towards the direction with large gradient. Gradient clipping heuristic algorithm has been proven successful in combating steep gradients common in recurrent neural networks.~\cite{Pascanu2012UnderstandingTE} 
    \item Poor Local Representation \\
Poor local representation can trick optimisation gradient to the wrong direction, especially when the direction of local minimum contradicts to global minimum. This can happen during exploring $\mathcal{Obj}(\phi)$ space for optimum solution and oppose challenges for model training. \\
Much of the efforts in research have been invested in finding the trouble-free optimisation start position that leads to convergence towards global optimum and avoids complex global structure. On the other hand, almost all gradient based algorithms are designed to optimise towards local extremes, minimum success has achieved in the attempt to develop algorithms that can escape the trap of local features, this still an active research area. \\
\end{enumerate}

