\paragraph{Deep neural network - Recurrent}
Recurrent neural networks (RNNs) is another class of neural networks, which have reputation in dealing with sequential data, particularly in places where data is independent to time steps. The connections between nodes in RNNs are not strictly unidirectional like in feed forward networks. In many RNN models, the feedback connections are to the past history, which allow RNNs to explore the correlations between sequential inputs.   
\par 
RNNs can also be viewed as state model of feed forward networks. One of the distinctive advantages of RNNs comparing to feed forward networks is that the RNNs can handle inputs with different dimensionalities, which is achieved via recurrent state definition as following:

\begin{equation}
    \mathrm{S}_k = \mathcal{f}(\mathrm{S}_{k-1} \cdot \mathrm{W} + \mathcal{x}_k \cdot \mathcal{w}_x)
\end{equation}
where $S_i$ is the state at time step $k$, $\mathrm{W}$ is recursive weight that is accumulated through layers, $\mathcal{x}_k$ is the training input vector and $\mathcal{w}_x$ is the weight matrix at current layer. The model output of $y_k$ at a given time step $\mathcal{k}$ can be computed from state definition. The recursive nature of state definition dictates delayed feedback, which allows RNNs to maintain memories from previous time steps. This recursive nature and delayed feedback loop has been demonstrated in in Figure~\ref{344423}. 
\par 
RNNs models are commonly trained via back propagation through time (BPTT)~\cite{Goodfellow-et-al-2016}, despite some more efficiency improved methods have been studied.~\cite{963769,neco.1989,Gomez:2008:ANE:1390681.1390712} In training RNN models, the nested network need to be flattened with respect to time steps
In addition, the local optimum trap is a far more pronounced issue with BPTT~\cite{Cu_llar}, 