One advantage of back propagation algorithm is that unlike other method, it can operate on non-normalized input vectors although additional normalization step will generally improve performance. \cite{Buckland:2002} The main theoretical challenge with gradient descent back propagation is that during optimisation, the gradient descent can stuck at local minimum and unable to reach the global minimum, this particularly concerning when error function is non-convex and the error surface is extremely rugged. Nevertheless,~\citet{LeCun_2015} argued this concern is largely theoretical and it is very unlikely to occur in majority practical applications.

Optimisation is an active developing field, new algorithms have been developed constantly. In the context of machine learning, gradient based algorithms are overwhelmingly popular, here we mainly focus discussion on those methods. In general, gradient-based algorithms are categorized as first order and second order, where first order algorithms use only gradient ($1^\circ$ derivative or first order) to improve model fitness while second order algorithms use both gradient and curvature ($2^\circ$ derivative or second order) information in the optimisation space to help make informative decision to obtain a better model over each iteration. \\

\paragraph {First Order Algorithms}
Optimisation in machine learning can be generalized as finding network configuration $\phi$ to make $\mathcal{Obj}(\phi)$ minimum. For a multi-dimensional input $\Phi$, directional gradient in the direction $\mathcal{u}$ defined by $\mathcal{u}^T\bigtriangledown_{\Phi}\mathcal{Obj}(\Phi)$. Directional gradient provides information about the changing direction of $\mathcal{Obj}(\Phi)$ in a direction $\mathcal{u}$, to minimise $\mathcal{Obj}(\Phi)$, we can decrease $\mathcal{Obj}$ towards direction that offer most negative gradient, which gives:
\begin{equation}
    \phi_{new} = \phi - \mu\bigtriangledown_{\phi}\mathcal{Obj}(\phi)
\end{equation}
where $\mu$ is learning rate, a definable scaler, which determines the step size for each iteration. This method is commonly referred as gradient descent or method of steepest descent~\cite{Debye_1909} and this concept is at the basis for almost all gradient based algorithms. \\
As a convention, the collection of all gradients (partial derivatives) for $\mathcal{Obj}:\mathbb{R}^m \to \mathbb{R}^n$ from different plans is referred as Jacobian matrix $\mathcal{J} \in \mathbb{R}^{m \times n}$ in machine learning, which is defined by:
\begin{equation}
    \mathcal{J}_{i,j} = \frac{\partial}{\partial \phi_j}\mathcal{Obj}(\phi)_i
\end{equation}

\begin{enumerate}
    \item Stochastic Gradient Descent (SGD)\\
SGD is arguably the most popular optimization algorithm due to its easy concept, relatively low computation burden and relatively fast training speed. Compare to gradient descent, which has a constant gradient over entire training dataset and issue one  model parameter $\phi$ update per epoch; SGD updates model parameter $\phi$ on per sample basis, which results much accelerated training. The training gradient in SGD for the training set with the size of $m$ is updated after each training sample $i$, which adopts the form: 
\begin{equation}
    g \approx \frac{1}{m}H_{\phi}\sum_{i=1}^m\mathcal{Obj}(\phi)_i
\end{equation}
The model parameter $\phi$ is updated after each sample evaluation by $\phi_{i+1} = \phi_i - \mu g$. In a convex space, the gradient points towards minimum is decreasing after each training, it is common practise to adopt a decreased learning rate $\mu_i$ over each iteration. Major benefits by adopting vanishing gradient in SGD include: preventing overshooting the optimum and avoid 
gradient on each training epoch 
    \item \\
\end{enumerate}

\paragraph{Second Oder Algorithms}
In addition to gradients, curvature (second derivative) can provide addition information on the changing direction of gradient. Hessian matrix $\mathrm{H}$ is defined as Jacobian of the gradient and takes form as 
\begin{equation}
    \mathrm{H}(f)(x)_{i,j} = \frac{\partial^2}{\partial x_i \partial x_j}f(x)
\end{equation}
In the context of machine learning, Hessian matrix is often real and symmetric. In multi-dimensional input evaluation, the curvature at direction $\mathcal{u}$ is given by $\mathcal{u}^TH\mathcal{u}$. Therefore, the evaluation of $f(x)$ around current $x_0$ under Taylor approximation gives:
\begin{equation}
    \label{eq:TaylorEvaluation}
    f(x) \approx f(x_0)+(x-x_0)^Tg+\frac{1}{2}(x-x_0)^TH(x-x_0)
\end{equation}
where $g$ is the gradient at $x_0$, $H$ is the curvature at $x_0$. If training rate is $\mu$, current evaluation point is given by $x_0-\mu\mathcal{g}$, the evaluation of $x_0-\mu\mathcal{g}$ is defined by:

\begin{equation}
\label{eq:TaylorEvaStep}
    f(x_0-\mu\mathcal{g}) \sim f(x_0) - \mu g^Tg + \frac{1}{2}\mu^2g^T\mathrm{H}g
\end{equation}
In order to avoid evaluation issues (more discussion as below) and improve evaluation accuracy, training rate $\mu$ is purposefully restricted to small value. We are looking for minimum, so when $g^THg > 0$, the optimum training rate $\mu_{optimum}$ is given by:

\begin{equation}
    \mu_{optimum} = \frac{g^Tg}{g^THg}
\end{equation}