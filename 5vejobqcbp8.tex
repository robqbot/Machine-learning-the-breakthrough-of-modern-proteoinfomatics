One advantage of back propagation algorithm is that unlike other method, it can operate on non-normalized input vectors although additional normalization step will generally improve performance. \cite{Buckland:2002} The main theoretical challenge with gradient descent back propagation is that during optimisation, the gradient descent can stuck at local minimum and unable to reach the global minimum, this particularly concerning when error function is non-convex and the error surface is extremely rugged. Nevertheless,~\citet{LeCun_2015} argued this concern is largely theoretical and it is very unlikely to occur in majority practical applications.

Optimisation is an active developing field, new algorithms have been developed constantly. In the context of machine learning, gradient based algorithms are overwhelmingly popular, here we mainly focus discussion on those methods. In general, gradient-based algorithms are categorized as first order and second order, where first order algorithms use only gradient ($1^\circ$ derivative or first order) to improve model fitness while second order algorithms use both gradient and curvature ($2^\circ$ derivative or second order) information in the optimisation space to help make informative decision to obtain a better model over each iteration. \\

\paragraph {First Order Algorithms}
Optimisation in machine learning can be generalized as finding network configuration $\phi$ to make $\mathcal{Obj}(\phi)$ minimum. For a multi-dimensional input $\Phi$, directional gradient in the direction $\mathcal{u}$ defined by $\mathcal{u}^T\bigtriangledown_{\Phi}\mathcal{Obj}(\Phi)$. Directional gradient provides information about the changing direction of $\mathcal{Obj}(\Phi)$ in a direction $\mathcal{u}$, to minimise $\mathcal{Obj}(\Phi)$, we can decrease $\mathcal{Obj}$ towards direction that offer most negative gradient, which gives:
\begin{equation}
    \phi_{new} = \phi - \mu\bigtriangledown_{\phi}\mathcal{Obj}(\phi)
\end{equation}
where $\mu$ is learning rate, a definable scaler, which determines the step size for each iteration. This method is commonly referred as gradient descent or method of steepest descent~\cite{Debye_1909} and this concept is at the basis for almost all gradient based algorithms.  

\begin{equation}
    
\end{equation}

\begin{enumerate}
    \item Stochastic Gradient Descent \\
    \item \\
\end{enumerate}

\paragraph{Second Oder Algorithms}
Hessian matrix $\mathrm{H}$ is defined as Jacobian of the gradient, which represents curvature of the $\mathcal{Obj}(\phi)$ and takes form as 
\begin{equation}
    \mathrm{H}(f)(x)_{i,j} = \frac{\partial^2}{\partial x_i \partial x_j}f(x)
\end{equation}
