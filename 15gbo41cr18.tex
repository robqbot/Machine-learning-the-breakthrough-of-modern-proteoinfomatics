\subsubsection{$L^1$ and $L^2$ Regularization}

The main purpose of regularization term is to reduce model complexity and help to avoid model overfitting problem by penalising the overly complex models. There are a few types of regularization that are popular, $L^1$ and $L^2$ are among the preferred choices, but the noticeable difference persist between the two. $L_1$ adopts form:

\begin{equation}
   \Omega(\phi) = \sum \parallel \mathcal{w} \parallel
\end{equation}

where $\mathcal{w}$ indicates all node weights that are affected by the regularization. 
\par 
The gradient of $L^1$ regularization is non-linear sign function $sign (\mathcal{w})$, which does not has a trivial algebraic solution

$L^2$ has the form:
\begin{equation}
   \Omega(\phi) = \frac {1}{2} \parallel \mathcal{w} \parallel^2
\end{equation}
$L^2$ regularization also commonly referred as weight decay, ridge regression or Tikhonov regularization. 