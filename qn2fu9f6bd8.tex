\subsection{Concepts of ANN models}
One of the learning algorithm is artificial neural networks (ANN) as demonstrated in Figure~\ref{147379}. The very first of ANN was proposed by~\citet{McCulloch_1943}, which was inspired by the structure of neural connections. Since 1960s, the discovery about visual information is processed through a layer by layer fashion in cats visual cortex~\cite{Hubel_1959,Hubel_1962}, several studies~\cite{Weng,Weng_1992,Weng_1997,Wenga,Riesenhuber_1999,Viglione_1970} have developed various ANN models in attempt to extract features from visual objects, however, ANN has only start to show promising performance and potentials since 2006 ~\cite{Hinton_2006a, Hinton_2006_1}. 
\par 
A typical ANN model construction possesses three structural layers, namely: one input layer, one or more hidden layers and one output layer. Each layer can have one or more nodes and each node is commonly referred as a neuron. Neurons in the each layer are either connected or partially connected with neurons at neighbour layers. The output value $O_o$ can be presented in inputs form as 

\begin{equation}
   O_o = g( \sum_{j} \sum_{i} B2*B1*W_{ij} * I_i) 
\end{equation}

where $I_i$ represents the input layer with i nodes; $O_o$ is the output layer with o nodes; $W_{i}$ is the weight of each input node $I_i$ in representing hidden node $H_j$; $W_{j}$ is the weight of each hidden node $H_j$ in representing output node $O_o$;  B1 and B2 are bias layers that apply constant values to the nodes at each subsequent layers after input layer; g is the activation function and generally is a non-linear function, sigmoid and Gaussian functions are commonly used for this purpose. The representation here is generalised based on single hidden layer.

\subsubsection{Actiation functions~\textit{g}}
The activation functions are critical component of ANN models, which simulate the transformation from input layer to output layer, that is generally non-linear in nature~\cite{LeCun_2015}. Various studies~\cite{Bengio_2012,Singh_2013} have demonstrated the importance of selecting proper activation function for a given scenario, which can lead to improved feature extraction. There are a few classes of commonly used activation functions as follows:
\begin{enumerate}
\item Sigmoid function \\
Sigmoid function is often characterised by its sigmoid curve, which is bounded by a pair of horizontal asymptotes as $x$ reaches infinity. As an activate function, sigmoid function often  it often take the forms either logistic function 
\begin{equation}
    g(x) = \frac{1} {1+ e^{-x}}
\end{equation}
or  and scaled logistic function, also known as hyperbolic tangent
\begin{equation}
    g(x) = tanh(x) = \frac{e^x-e^{-x}} {e^x + e^{-x}}
\end{equation}

\item Softmax \\
Softmax returns a probability vector, which sums to 1. This property is applicable in transforming logits scores (i.e.  categorical distribution) into probability and the implementation largely occurs at the output layer. Softmax function is also known as normalized exponential function~\cite{Howard_2007}, which adapts the form: 
\begin{equation}
    g(x) = \frac{e^{x_i}} {\sum_k e^{x_k}}
\end{equation}
where $x_i$ represents each element of input vector $X$ with a total of $k$ number of elements. 
\item Maxout \\

\end{enumerate}