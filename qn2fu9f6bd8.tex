\subsection{Concepts of ANN models}
One of the learning algorithm is artificial neural networks (ANN) as demonstrated in Figure~\ref{147379}. The very first of ANN was proposed by~\citet{McCulloch_1943}, which was inspired by the structure of neural connections. Since 1960s, the discovery about visual information is processed through a layer by layer fashion in cats visual cortex~\cite{Hubel_1959,Hubel_1962}, several studies~\cite{Weng,Weng_1992,Weng_1997,Wenga,Riesenhuber_1999,Viglione_1970} have developed various ANN models in attempt to extract features from visual objects, however, ANN has only start to show promising performance and potentials since 2006 ~\cite{Hinton_2006a, Hinton_2006_1}. 
\par 
A typical ANN model construction possesses three structural layers, namely: one input layer, one or more hidden layers and one output layer. Each layer can have one or more nodes and each node is commonly referred as a neuron. Neurons in the each layer are either connected or partially connected with neurons at neighbour layers. The output value $O_o$ can be presented in inputs form as 

\begin{equation}
   O_o = g( \sum_{j} \sum_{i} B2*B1*W_{ij} * I_i) 
\end{equation}

where $I_i$ represents the input layer with i nodes; $O_o$ is the output layer with o nodes; $W_{i}$ is the weight of each input node $I_i$ in representing hidden node $H_j$; $W_{j}$ is the weight of each hidden node $H_j$ in representing output node $O_o$;  B1 and B2 are bias layers that apply constant values to the nodes at each subsequent layers after input layer; g is the activation function and generally is a non-linear function, sigmoid and Gaussian functions are commonly used for this purpose. The representation here is generalised based on single hidden layer.

\subsubsection{Actiation functions~\textit{g}}
The activation functions are critical component of ANN models, which simulate the transformation from input layer to output layer, that is generally non-linear in nature~\cite{LeCun_2015}. Various studies~\cite{Bengio_2012,Singh_2013} have demonstrated the importance of selecting proper activation function for a given scenario, which can lead to improved feature extraction. There are a few classes of commonly used activation functions as follows:
\begin{enumerate}
\item Sigmoid function \\
Sigmoid function is often characterised by its sigmoid curve, which is bounded by a pair of horizontal asymptotes as $x$ reaches infinity. As an activate function, sigmoid function can be applied at all layers and it often take the forms of either logistic function:
\begin{equation}
    g(x) = \frac{1} {1+ e^{-x}}
\end{equation}
or scaled logistic function, commonly adopts the form of hyperbolic tangent
\begin{equation}
    g(x) = tanh(x) = \frac{e^x-e^{-x}} {e^x + e^{-x}} = \frac {2} {1 + e^{-2x}} = 2 sigmoid(2x) - 1
\end{equation}

\item Softmax \\
Softmax returns a probability vector, which sums to 1. This property is applicable in transforming logits scores (i.e.  categorical distribution) into probability and the implementation largely occurs at the output layer. Softmax function is also known as normalized exponential function~\cite{Howard_2007}, which adapts the form: 

\begin{equation}
    g(x) = \frac{e^{x_i}} {\sum_k e^{x_k}}
\end{equation}

where $x_i$ represents each element of input vector $X$ with a total of $k$ number of elements. 

\item Rectified Linear Unit (RELU) \\
RELU adopts form as:

\begin{equation}
    g(x) = \max(0,x)
\end{equation}

RELU function has threshold at zero, which is only activated when $x > 0$ and increase linearly thereafter with slope 1.~\citet{Krizhevsky_2017} has found greater stochastic gradient descent convergence rate with RELU up to 6 times faster when compared to sigmoid activation functions. In addition, the implementation of RELU is much less computational heavy than sigmoid (i.e. involving exponential operations), whereas RELU can be implemented via an activation matrix at threshold zero. Therefore, RELU is arguably the most wildly implemented activation function for ANN models.~\cite{Bengio_2012,Toth_2013,Jaitly_2011,nga}  Nevertheless, RELU is not free without issues. Although the stochastic gradient convergence property of RELU is greater than most of the activation functions, RELU is less perceptible to aggressive learning rate. This adverse effect is observable when a too steep convergence gradient passing through RELU neurons in training, which can cause the contributing weight of such neurons to deadlock to zero. It is not unheard of that substantial portion of RELU activated neurons in the network are deadlocked during training, which can result a zombie network. 
\par 
There are a few attempts to correct such deadlock vulnerability in RELU by create RELU variant models, noticeably, leaky RELU and softplus are among those variants. Leaky RELU prevents deadlock during training by assigning a small, negligible positive gradient to dormant neurons, which adopts the form: 

\begin{equation}
    g(x) = \left\{ 
    \begin{array}{lr}
        x & \text{for } x > 0 \\
        a_cx & \text{for } x\leq 0
    \end{array}\right
\end{equation}

\item Maxout \\

\end{enumerate}