\subsection{Concepts of ANN models}
One of the learning algorithm is artificial neural networks (ANN) as demonstrated in Figure~\ref{147379}. The very first of ANN was proposed by~\citet{McCulloch_1943}, which was inspired by the structure of neural connections. Since 1960s, the discovery about visual information is processed through a layer by layer fashion in cats visual cortex~\cite{Hubel_1959,Hubel_1962}, several studies~\cite{Weng,Weng_1992,Weng_1997,Wenga,Riesenhuber_1999,Viglione_1970} have developed various ANN models in attempt to extract features from visual objects, however, ANN has only start to show promising performance and potentials since 2006 ~\cite{Hinton_2006a, Hinton_2006_1}. 
\par 
A typical ANN model construction possesses three structural layers, namely: one input layer, one or more hidden layers and one output layer. Each layer can have one or more nodes and each node is commonly referred as a neuron. Neurons in the each layer are either connected or partially connected with neurons at neighbour layers. The output value $O_o$ can be presented in inputs form as 

\begin{equation}
   O_o = g( \sum_{j} \sum_{i} B2*B1*W_{ij} * I_i) 
\end{equation}

where $I_i$ represents the input layer with i nodes; $O_o$ is the output layer with o nodes; $W_{i}$ is the weight of each input node $I_i$ in representing hidden node $H_j$; $W_{j}$ is the weight of each hidden node $H_j$ in representing output node $O_o$;  B1 and B2 are bias layers that apply constant values to the nodes at each subsequent layers after input layer; g is the activation function and generally is a non-linear function, sigmoid and Gaussian functions are commonly used for this purpose. The representation here is generalised based on single hidden layer.

\subsubsection{Actiation functions~\textit{g}}
The activation functions are critical component of ANN models, which simulate the transformation from input layer to output layer, that is generally non-linear in nature~\cite{LeCun_2015}. Various studies~\cite{Bengio_2012,Singh_2013} have demonstrated the importance of selecting proper activation function for a given scenario, which can lead to improved feature extraction. There are a few classes of commonly used activation functions as follows:
\begin{enumerate}
\item Sigmoid function \\
Sigmoid function is often characterised by its sigmoid curve, which is bounded by a pair of horizontal asymptotes as $x$ reaches infinity. Historically, sigmoid function has been commonly acted as activation function, as it transform input from open range continuous real numbers to a value range from 0 to 1, it often take the forms of either logistic function:

\begin{equation}
    g(x) = \frac{1} {1+ e^{-x}}
\end{equation}

or scaled logistic function, which commonly adopts the form of hyperbolic tangent

\begin{equation}
    g(x) = tanh(x) = \frac{e^x-e^{-x}} {e^x + e^{-x}} = \frac {2} {1 + e^{-2x}} = 2 sigmoid(2x) - 1
\end{equation}

However, as Figure shown, logistic function plateaus at either 0 or 1, which results nodes saturation around plateau and convergence gradients at both regions are approaching 0, which results an inefficient learning. Furthermore, logistic output $g(x) \geq 0$, this will only allow the gradient descent on the same side, which in turn may cause zigzag effect during training and slow the convergence. 

\item Softmax \\
Softmax returns a probability vector, which sums to 1. This property is applicable in transforming logits scores (i.e.  categorical distribution) into probability and the implementation largely occurs at the output layer. Softmax function is also known as normalized exponential function~\cite{Howard_2007}, which adapts the form: 

\begin{equation}
    g(x) = \frac{e^{x_i}} {\sum_k e^{x_k}}
\end{equation}

where $x_i$ represents each element of input vector $X$ with a total of $k$ number of elements. 

\item Rectified Linear Unit (RELU) \\
RELU adopts form as:

\begin{equation}
    g(x) = \max(0,x)
\end{equation}

RELU function has threshold at zero, which is only activated when $x > 0$ and increase linearly thereafter with slope 1.~\citet{Krizhevsky_2017} has found greater stochastic gradient descent convergence rate with RELU up to 6 times faster when compared to sigmoid activation functions. In addition, the implementation of RELU is much less computational heavy than sigmoid (i.e. involving exponential operations), whereas RELU can be implemented via an activation matrix at threshold zero. Therefore, RELU is arguably the most wildly and most popular implemented activation function for ANN models.~\cite{Bengio_2012,Toth_2013,Jaitly_2011,nga,LeCun_2015,}  Nevertheless, RELU is not free without issues. Although the stochastic gradient convergence property of RELU is greater than most of the activation functions, RELU is less perceptible to aggressive learning rate. This adverse effect is observable when a too steep convergence gradient passing through RELU neurons in training, which can cause the contributing weight of such neurons to deadlock to zero. It is not unheard of that substantial portion of RELU activated neurons in the network are deadlocked during training, which can result a zombie network. 
\par 
There are a few attempts to correct such deadlock vulnerability in RELU by create RELU variant models, noticeably, leaky RELU and softplus are among those variants. Leaky RELU prevents deadlock during training by assigning a small, negligible positive gradient to dormant neurons, which adopts the form: 

\begin{equation}
    g(x) = \left\{ 
    \begin{array}{lr}
        x & \text{for } x > 0 \\
        0.01x & \text{for } x\leq 0
    \end{array}\right.
\end{equation}

on the basis of leaky RELU,~\citet{He_2015} has introduced a Parametric Rectified Linear Unit (PReLU), which adopts following form:

\begin{equation}
    g(x) = \left\{ 
    \begin{array}{lr}
        x & \text{for } x > 0 \\
        a_{i}x & \text{for } x\leq 0
    \end{array}\right.
\end{equation}

where $a_{i}$ represents leakage parameter, which is generally small and trainable for each node (neuron). Nonetheless, the consistent benefits of PReLU need further investigation. 
\par 
Softplus function is also referred as SmoothReLU function, which is another variation of RELU. As name suggested, softplus smooth out the approximation of RELU from negative to positive range, this smooth effect is particular noticeable at boundary $x=0$, which in turn, softplus function is differentiable throughout range. Softplus function adopts the form of   

\begin{equation}
    g(x) = ln(1+e^{x})
\end{equation}

\item Maxout \\
\citealt{Goodfellow:2013:MN:3042817.3043084} has introduced a maxout neuron, which adopts the form:

\begin{equation}
    g_i(x) = \max_{j\in{1,k}}(z_{ij}), z_{ij} = x^T \cdot W_{ij} + b_{ij} \iff
    \mathcal{G}(\vec{X}) = max(\mathcal{W}^T\vec{X} + \mathcal{B}) 
\end{equation}

where $x\in\mathbb{R}^d$, that can be either input vector $\upsilon$ or previous hidden layer's state vector $\mathcal{p}$. Both weight matrix $\mathcal{W}\in\mathbb{R}^{d,m,k}$ and bias matrix $\mathcal{B}\in\mathbb{R}^{m,k}$ are trainable. The third dimension of the weight matrix $\mathcal{B}
\end{enumerate}