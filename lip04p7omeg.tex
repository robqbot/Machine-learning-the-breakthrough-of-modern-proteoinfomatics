\subsubsection{Common Deep Learning Architectures}

\paragraph{Deep neural network - Forwarding}
Feedforward neural networks also known as multilayer perceptrons (MLPs) and are the archetypal of all machine learning models. The information flow in those models is unidirectional and always forward from input $\mathcal{x}$ to intermediate layers (often defined as models $\mathcal{f}(\phi)$ (where $\phi$ is model parameters), eventually spit out the prediction $\mathcal{y}$. There is no feedback instrumentations that allows output to re-enter the model $\mathcal{f}(\phi)$. The ultimate target to achieve in feedforward neural networks is to approximate a model function parameter $\phi$ that give arise to a model that predict a close $\mathcal{y}$ that is close to observation $\hat\mathcal{y}$ for a given input $\mathcal{x}$. 
\par 
One type of forward neural network that has received most of attention recently is convolution neural network. 

To better elucidate the evolution of feedforward neural networks. We will consider a linear model, say linear or logistic regression models for example. Linear models are easy to evaluate and optimise however they lack of representation that is between variables. In order to adapt linear models for non-linear relationships, a non-linear transformation function $\psi$ often employed, hence model $\mathcal{f}$ can be approximated against $\psi(x)$. As machine learning advances, there are a few options to introduce proper $\psi$: 

\begin{enumerate}
    \item Generic approach \\
Where an infinite-dimensional $\psi$ can be erected such that $\psi(x)$ is high enough in dimensions to fit for all features contained in training dataset. However, the generalisation is largely poor and largely dictated by the local features, generally incapable of solving complex problems. 
    \item Heuristic manual engineering \\
Heuristic manual approach~\cite{Goodfellow-et-al-2016} was used to be dominant approach prior to modern deep learnings. The process is long and often tedious, which requires advanced domain knowledge and lack of transferability across disciplines.    
    \item Deep learning\\
With modern deep learning algorithms, we can now define learning problem as 
\begin{equation}
    \mathcal{y} = \mathcal{f}(x; \phi, \mathcal{w}) = \psi(\mathcal{x}; \phi)^T\mathcal{w}
\end{equation}
where model parameter $\phi$ can be trained from a broad spectrum of functions. $\mathcal{w}$ is a correlation matrix that convert $\mathcal{x}$ transformation $\psi(\mathcal{x}; \phi)$ into linear correspondent $\mathcal{y}$.  \\
$\psi$ can be highly generic if desire so and expert knowledge can also be adapted by selecting a proper class in corresponding to the problem domain. 
\end{enumerate}

\paragraph{Deep neural network - Recurrent}
Recurrent neural networks (RNNs) has reputation in dealing with sequential data, particular in places where data is independent to time steps. 

RNNs models can be trained via back propagation through time~\cite{Goodfellow-et-al-2016}, although some efficiency improved methods have been studied.~\cite{963769,neco.1989,Gomez:2008:ANE:1390681.1390712}

\paragraph{Convolutional neural network}
Convolutional neural network (CNN) has shown promising implementation in image recognition. The composition of a typical CNN consists several convolution layers and sub-sampling layers in tenden. Each convolution layer is a filter, commonly referred as kernel, which possess a few receptive fields and each is programmable through parameters.  
Convolution neural network has shown far superior performance in image recognition than all other algorithms to date.~\cite{Szegedy_2015}
\par 
Within the realm of convolutional neural networks, there are three