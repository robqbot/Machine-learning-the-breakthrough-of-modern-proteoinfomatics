\subsubsection{Common Deep Learning Architectures}

\paragraph{Deep neural network - Forwarding}
Feedforward neural networks also known as multilayer perceptrons (MLPs) and are the archetypal of all machine learning models. The information flow in those models is unidirectional and always forward from input $\mathcal{x}$ to intermediate layers (often defined as models $\mathcal{f}(\phi)$ (where $\phi$ is model parameters), eventually spit out the prediction $\mathcal{y}$. There is no feedback instrumentations that allows output to re-enter the model $\mathcal{f}(\phi)$. The ultimate target to achieve in feedforward neural networks is to approximate a model function parameter $\phi$ that give arise to a model that predict a close $\mathcal{y}$ that is close to observation $\hat\mathcal{y}$ for a given input $\mathcal{x}$. 
\par 
One type of forward neural network that has received most of attention recently is convolution neural network. 

To better elucidate the evolution of feedforward neural networks. We will consider a linear model
\paragraph{Deep neural network - Recurrent}
Recurrent neural networks (RNNs) has reputation in dealing with sequential data, particular in places where data is independent to time steps. 

RNNs models can be trained via back propagation through time~\cite{Goodfellow-et-al-2016}, although some efficiency improved methods have been studied.~\cite{963769,neco.1989,Gomez:2008:ANE:1390681.1390712}

\paragraph{Convolutional neural network}
Convolutional neural network (CNN) has shown promising implementation in image recognition. The composition of a typical CNN consists several convolution layers and sub-sampling layers in tenden. Each convolution layer is a filter, commonly referred as kernel, which possess a few receptive fields and each is programmable through parameters.  
Convolution neural network has shown far superior performance in image recognition than all other algorithms to date.~\cite{Szegedy_2015}
\par 
Within the realm of convolutional neural networks, there are three