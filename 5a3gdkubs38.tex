\subsubsection{Loss Function}

Most common form of loss function is arguably cross entropy, which measures the discrepancy between model prediction and training data. Due to most of the modern neural network adopts back propagation algorithm to train, the loss function therefore has to be at least 1\textdegree differentiable to allow error gradient to be calculated. The loss function can be generalized in the form:

\begin{equation}
    E = \frac {1}{n} \sum_x^n E_x
\end{equation}

where $E_x