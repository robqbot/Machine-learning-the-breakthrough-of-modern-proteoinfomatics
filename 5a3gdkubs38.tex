Loss functions are used to measure the discrepancy between model predictions and training datasets. Loss functions broadly divided into two categories: classification and regression. As the cross entropy loss (in machine learning also known as log loss) arguably the most popular loss function choice for classification models that commonly behind the deep learning applications in image recognitions, Mean Square Error (MSE) on the other hand, is probably the most well-known regression loss function. 
\par 
Most of the modern neural networks adopt back propagation algorithm for model training (more about model trainings in later section), the loss function therefore has to be at least 1\textdegree differentiable to allow error gradient to be calculated. The loss function can be generalized in the form:

\begin{equation}
    E = \frac {1}{n} \sum_x^n E_x
\end{equation}

where $E_x$ denotes the error occurred on a training on dataset $x$ with $n$ number of training points. If the error has been measured in the square of Euclidean distance:

\begin{equation}
    E(\mathcal{I}) = \frac {1}{2n} \sum_x^n \parallel y(\mathcal{I}) - y^*(\mathcal{I}) \parallel^2 
\end{equation}

where $\mathcal{x}$ is the input vector, $y^*(\mathcal{I})$ is the actual output with given input vector $\mathcal{I}$, $y(\mathcal{I})$ is the model predicted output with given $\mathcal{I}$. 
\par 
We here discuss a few common loss functions from each classification and regression categories. Although the classification models can be used to predict a continuous value as probability of a class label, classification models more often used for predicting a discrete categories, classes or labels a given input belongs to using past experience. 

\paragraph {Loss functions for Classification models}
Common loss functions used for classification models include:

\begin{enumerate}
    \item Hinge Loss/Multi-class Support Vector Machines (SVMs) Loss \\
Hinge Loss commonly used for max-margin classification, Hinge loss works well as a training classifier in SVMs~\cite{Cortes_1995}. Given the intended output from SVMs $\hat{y} = \pm m$, in binary classification $m = 1$, $y$ is the prediction, the hinge loss of prediction $y$ given by:
\begin {equation}
    \mathcal{L}(y) = \frac {1}{n} \sum_i^n max(0,m - y_i \cdot \hat{y})
\end{equation}
It is clear that the penalty on the hinge loss increases linearly as the prediction $y$ deviates from true value $\hat{y}$.
    \item Exponential Loss \\
Similar to hinge loss, the exponential loss impose more severe penalty on the model if the prediction is not consist with expectancy. The exponential loss is given by:
\begin {equation}
    \mathcal{L}(y) = \frac {1}{n} \sum_i^n e^{-\lambda y \cdot \hat{y}}
\end{equation}
    \item Cross Entropy \\
Cross entropy is also known as log loss and is widely applicable in binary classification, which is arguably the most used loss function in visual pattern recognition with convolutional neural networks. Here, we demonstrate the binary setup, where two class can be defined as 
\begin{equation}
   p = \left\{ 
    \begin{array}{lr}
        log(\mathrm{prob}(y_i)) & y_i = 1~\text{(i.e.} y_i~\text{in positive class)}  \\
        log(\mathrm{prob}(1-y_i))y_i & y_i = 0 ~\text{(i.e.} y_i~\text{in negative class)}   \\
    \end{array}\right.
  
defined as:
\begin{equation}
    \mathcal{L} = -\frac{1}{n}\sum_{i=1}^n [y_i \times log\hat{y_i} +(1-y_i) \times log(1-\hat{y_i}) ] 
\end{equation}

The function measure the divergence between the distribution of predicted values and distribution of observed values. One advantage of cross entropy loss is that this algorithm provides fast convergence and allows quick learning, this is particularly advantageous for neurons constructed with sigmoid activation functions, as state earlier sigmoid function can converge slowly under some circumstances.\\
    \item Kullback Leibler (KL) Divergence\\
KL divergence better known as relative entropy, is a probabilistic measurement on the divergence of predicted distribution to expected distribution. Given that the prediction has the distribution of $p$ and the training data has the distribution of $q$, KL divergence can be interpreted as the difference between cross entropy $\mathbf{H}_p(q)$ and entropy of training data $\mathbf{H}(q)$ which can take the form of:
\begin{equation}
    \begin{aligned}
    \mathcal{L} &= \mathcal{D}_{KL}(p \parallel q) = \mathbf{H}_p(\mathrm{q}) - H(\mathrm{q}) \\
    &= - \frac {1}{n} \sum_i^n \mathrm{p}_i \times log \mathrm{q}_i + \frac{1}{n}\sum_{i=1}^n [\mathrm{p}_i \times log \mathrm{p}_i] 
    \end{aligned}
\end{equation}
\end {enumerate}