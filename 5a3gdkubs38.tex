\subsubsection{Loss Function}

Most common form of loss function is arguably cross entropy, which measures the discrepancy between model prediction and training data. Due to most of the modern neural network adopts back propagation algorithm to train, the loss function therefore has to be at least 1\textdegree differentiable to allow error gradient to be calculated. The loss function can be generalized in the form:

\begin{equation}
    E = \frac {1}{n} \sum_x^n E_x
\end{equation}

where $E_x$ denotes the error occurred on a training on dataset $x$ with $n$ number of training points. If the error has been measured in the square of Euclidean distance:

\begin{equation}
    E(\mathcal{I}) = \frac {1}{2n} \sum_x^n \parallel y(\mathcal{I}) - y^*(\mathcal{I}) \parallel^2 
\end{equation}

where $\mathcal{x}$ is the input vector, $y^*(\mathcal{I})$ is the actual output with given input vector $\mathcal{I}$, $y(\mathcal{I})$ is the model predicted output with given $\mathcal{I}$. 
\par 
A few popular 
\par 
One advantage of back propagation algorithm is that unlike other method, it can operate on non-normalized input vectors although additional normalization step will generally improve performance. \cite{Buckland:2002} The main theoretical challenge with gradient descent back propagation is that during optimisation, the gradient descent can stuck at local minimum and unable to reach the global minimum, this particularly concerning when error function is non-convex and the error surface is extremely rugged. Nevertheless,~\citet{LeCun_2015} argued this concern is theoretical and is very unlikely for many real practical applications. 