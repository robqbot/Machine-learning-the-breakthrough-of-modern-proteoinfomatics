\subsubsection{Loss Function}

Most common form of loss function is arguably cross entropy, which measures the discrepancy between model prediction and training data. Due to most of the modern neural network adopts back propagation algorithm to train, the loss function therefore has to be at least 1\textdegree differentiable to allow error gradient to be calculated. The loss function can be generalized in the form:

\begin{equation}
    E = \frac {1}{n} \sum_x^n E_x
\end{equation}

where $E_x$ denotes the error occurred on a training on dataset $x$ with $n$ number of training points. If the error has been measured in the square of Euclidean distance:

\begin{equation}
    E(\mathcal{I}) = \frac {1}{2n} \sum_x^n \parallel y(\mathcal{I}) - y^*(\mathcal{I}) \parallel^2 
\end{equation}

where $\mathcal{x}$ is the input vector, $y^*(\mathcal{I})$ is the actual output with given input vector $\mathcal{I}$, $y(\mathcal{I})$ is the model predicted output with given $\mathcal{I}$. 
\par 
A few common loss functions that used in machine learning include:

\begin{enumerate}
    \item Mean Squared Error (MSE) \\
MSE has the form:
\begin{equation}
    MSE = \frac{1}{n}\sum_{i=1}^n (\hat{x}_i - x_i)^2
\end{equation}
where $\hat{x}_i$ is the estimator and $x_i$ is the actual value. 
    \item Likehood Loss \\
Likehood loss is commonly used in classification models. Although the the approach is an empirical, it is helpful in comparing model performance. 
    \item Cross Entropy \\
Cross entropy is also known as log loss and is widely applicable in binary classification, which is extremely popular in visual pattern recognition application with convolutional neural network, which has the form:
\begin{equation}
    \mathcal{L} = -\frac{1}{n}\sum_{i=1}^n [y_i \times log\hat{y_i} +(1-y_i) \times log(1-\hat{y_i}) ] 
\end{equation}

The function measure the divergence between the distribution of predicted values and distribution of observed values. One advantage of cross entry
\end{enumerate}

\par 
One advantage of back propagation algorithm is that unlike other method, it can operate on non-normalized input vectors although additional normalization step will generally improve performance. \cite{Buckland:2002} The main theoretical challenge with gradient descent back propagation is that during optimisation, the gradient descent can stuck at local minimum and unable to reach the global minimum, this particularly concerning when error function is non-convex and the error surface is extremely rugged. Nevertheless,~\citet{LeCun_2015} argued this concern is theoretical and is very unlikely for many real practical applications. 