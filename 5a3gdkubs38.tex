Loss functions are used to measure the discrepancy between model predictions and training datasets. Loss functions broadly divided into two categories: classification and regression. As the cross entropy loss (in machine learning also known as log loss) arguably the most popular loss function choice for classification models that commonly behind the deep learning applications in image recognitions, Mean Square Error (MSE) on the other hand, is probably the most well-known regression loss function. 
\par 
Most of the modern neural networks adopt back propagation algorithm for model training (more about model trainings in later section), the loss function therefore has to be at least 1\textdegree differentiable to allow error gradient to be calculated. The loss function can be generalized in the form:

\begin{equation}
    E = \frac {1}{n} \sum_x^n E_x
\end{equation}

where $E_x$ denotes the error occurred on a training on dataset $x$ with $n$ number of training points. If the error has been measured in the square of Euclidean distance:

\begin{equation}
    E(\mathcal{I}) = \frac {1}{2n} \sum_x^n \parallel y(\mathcal{I}) - y^*(\mathcal{I}) \parallel^2 
\end{equation}

where $\mathcal{x}$ is the input vector, $y^*(\mathcal{I})$ is the actual output with given input vector $\mathcal{I}$, $y(\mathcal{I})$ is the model predicted output with given $\mathcal{I}$. 
\par 
We here discuss a few common loss functions from each classification and regression categories. Although the classification models can be used to predict a continuous value as probability of a class label, classification models more often used for predicting a discrete categories, classes or labels a given input belongs to using past experience. 

