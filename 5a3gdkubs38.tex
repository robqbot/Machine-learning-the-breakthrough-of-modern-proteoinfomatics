\subsubsection{Loss Function}

Most common form of loss function is arguably cross entropy, which measures the discrepancy between model prediction and training data. Due to most of the modern neural network adopts back propagation algorithm to train, the loss function therefore has to be at least 1\textdegree differentiable to allow error gradient to be calculated. The loss function can be generalized in the form:

\begin{equation}
    E = \frac {1}{n} \sum_x^n E_x
\end{equation}

where $E_x$ denotes the error occurred on a training on dataset $x$ with $n$ number of training points. If the error has been measured in the square of Euclidean distance:

\begin{equation}
    E(\mathcal{I}) = \frac {1}{2n} \sum_x^n \parallel y(\mathcal{I}) - y^*(\mathcal{I}) \parallel^2 
\end{equation}

where $\mathcal{x}$ is the input vector, $y^*(\mathcal{I})$ is the actual output with given input vector $\mathcal{I}$, $y(\mathcal{I})$ is the model predicted output with given $\mathcal{I}$. 
\par 
One advantage of back propagation algorithm is that unlike other method, it can operat