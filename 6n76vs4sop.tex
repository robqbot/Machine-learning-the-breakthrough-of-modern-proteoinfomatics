\paragraph {Loss Functions for Regression models}
Regressional models predict continuous values
\begin{enumerate}
    \item Mean Squared Error (MSE) \\
MSE commonly referred as L2 loss and arguably the most popular form of loss function in regression models, which adopts the form:
\begin{equation}
    MSE = \frac{1}{n}\sum_{i=1}^n (\hat{x}_i - x_i)^2
\end{equation}
where $\hat{x}_i$ is the estimator and $x_i$ is the actual value. MSE is continuous and differentiable, which makes MSE particularly desirable for computation as gradient can be easily calculated. 
    \item Mean Absolute Error (MAE) \\
MAE is referred as L1 loss in statistics, similar to MSE, MAE is also a common form to measure model fitness with slight mathematical variation:
\begin{equation}
    \mathcal{L} = \frac {1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
\end{equation}
This variation however made MAE is difficult to calculate gradient and heavy on computation. Nevertheless, due to its linear representation, MAE does offer balanced penalty on outlier errors, MSE on the other hand, can be easily dominated by the heavy penalty on this region, which may create undesired penalty profile. As a thumb of rule, MAE is more tolerant to outlier prediction and therefore MAE can be used in scenario where outlier is present in the population. MAE particularly useful if training data is contaminated and outlier is abundant. 
    \item Huber Loss\\
Huber loss is introduced by~\citet{Huber_1964}, which is also known as smooth absolute error. Huber loss hybrids the advantages on both differentiability and tolerance on outlier, which is particularly useful in examine datasets with distinct cluster substructures. For example, a collection of local temperature profile over winter and summer with winter data consists majority; under which case, MAE lose enabled model will likely to tuning exclusively to winter profile due to its insensitivity to outlier, but MSE loss enabled models will likely to skew towards summer profile. Both MAE and MSE loss function enabled models are unable to provide clear insights about clustered structure. 
\par 
Huber loss adopts piecewise approach as: 

\begin{equation}
    \mathcal{L}_\delta = \left\{ 
    \begin{array}{ll}
        \frac {1}{2} (y - \hat{y})^2 & \text{for } |y-\hat{y}| \leq \delta, \\
        \delta |y - \hat{y}| - \frac{1}{2} \delta^2 & \text{otherwise.} 
    \end{array}\right.
\end{equation}
$\delta$ is Huber parameter, which defines trajectory of Huber loss function. As $\delta \rightarrow 0$, Huber loss becomes more MAE like and $\delta \rightarrow \infty$, Huber loss becomes more like MSE. The advantage of Huber loss is apparent at small error region defined by $\delta$, where Huber loss transits from absolute error to quadratic. 
    \item Log Cosh Loss \\
Log cosh loss is defined as its name suggested: logarithm of hyperbolic cosine, which has the form as:
\begin{equation}
    \mathcal{L} = \sum_{i=1}^n log[cosh(y_i-\hat{y_i})]
\end{equation}
where $\hat{y}$ is target and $y$ is model prediction. \\
Similar to Huber loss, log cosh loss is also less sensitive to outlier influence and the function is not only continuous but also $2^{\circ}$ differentiable. Therefore, Huber loss is among the popular choices in model fitness evaluation that adopts Newton-Raphson method, which uses curvature (Hessian) to determine a fitter solution. 
    \item Other forms\\
There are many loss forms in the literature with their own advantages like Mean Squared Logarithmic Error (MSLE), which adopts the form:

\begin{equation}
    \mathcal{L} = -\frac{1}{n}\sum_{i=1}^n [log (y_i + 1) -  log(\hat{y_i} + 1) ]^2 
\end{equation}

which has advantages in dealing with models that work with large absolute values, such scenario often attracts penalties during model training even the error percentage is low. \\
Quantile loss is another commonly used loss function, which is essentially an extension of MAE but provides a confident envelope around prediction. Quantile loss is valuable in model predicting intervals rather than a specific value. Most regression models assume the residuals between model prediction and targets $y-\hat{y}$ has a constant variance throughout, quantile loss enabled regression models is one of not many choices when such assumption is violated~\cite{2019arXiv190107874T, Huang2017}.
\end{enumerate}