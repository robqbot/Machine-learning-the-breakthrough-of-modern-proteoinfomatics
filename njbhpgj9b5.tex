Machine learning is generally categorized into three categories: supervised, unsupervised and reinforcement learning. The supervised learning focus on reaching a targeted outcome through given features or attributes. Unsupervised learning on the other hand, focus on clustering and grouping. Reinforcement learning adopts various agents with decision matrices and reward function to master a given action. Developing a winning algorithm for board games is a typical reinforcement learning task like the famous alpha Go.
\par

\subsubsection{Supervised Learning}

Recent popularity of deep learning, which largely driven by its superior performance in image pattern recognition applications, is a type of supervised learning models. The supervised learning  derives the relationship between input~\textit{x} and the output~\textit{y}, it comes with two major categories as probabilistic and non-probabilistic supervised learnings. 

\paragraph{Probabilistic based learning}
Most of modern supervised learning strategies, particularly the expansion of deep learning mechanisms, many are considered black-box approach, as we know little about the underlying mechanism how model works. Therefore, it is increasingly important to understand the statistical uncertainty associated with model predictions. Probabilistic based supervised learnings are designed to provide full spectrum of predictive outcomes. 
\par 

In linear regression models, model parameters can be learned using Bayesian estimation. Given model is trained as following

\begin{equation}
    \mathcal{y} = \mathrm{X}\mathcal{w}
\end{equation}
where $\mathrm{X}$ is input vector, $\mathcal{y}$ is model output, $\mathcal{w}$ is the parameter matrix. If $\mathcal{y}$ expressed as Gaussian conditional distribution, then it gives:

\begin{equation}
    \mathcal{p}(\mathcal{y}|\mathrm{X},\mathcal{w}) = \mathcal{N}(\mathcal{y};\mathrm{X}\mathcal{w},\mathrm{I})
\end{equation}

To generalize into classification models, we can define binary class distribution, namely zero and one. One way to classify a distribution over binary categories is to adopt logistic sigmoid function, which has an output between $(0,1)$ and this can be treated as a probability given as 

\begin{equation}
    \mathcal{p}(y=1|\mathcal{x};\phi) = \psi(\phi^T\mathcal{x})
\end{equation}
where $\phi$ is model parameter matrix and $\psi$ is an sigmoid function. Solving for optimum weight matrix 

\paragraph{Support Vector Machines (SVMs)}
SVMs are driven by linear function $\mathcal{w}^T\mathcal{x}+b$ 


\subsubsection{Unsupervised Learning}
Although the distinction between supervised and unsupervised learnings are not rigid due to the lack of quantitative measurement to quantify, the general consensus for defining unsupervised learnings associates with density estimation, where involve extracting features within dataset~\textit{x} then sorting data points in dataset~\textit{x} into feature categories.
Unsupervised learnings often involve finding a close representation of a big given  dataset, which preserves as much information in dataset~\textit{x} while much slimmer in size. In a broad term, three types of representations are commonly found, namely, sparse representation, independent representation and lower dimensional representation. The sparse representation~\cite{Barlow_1989,Olshausen_1996,Hinton_1997} has higher dimentionality compare to the other two to compensate the sparsity of the data volume, which likely to result the overall representation space concentrates along the representation axes. In independent representation, the representation are selected in such a way that they are statistically independent to each other. For lower dimensional representation, the efforts focus on reducing dimensionality of dataset for but largely preserve features at the same time. 

\paragraph{Principal Components Analysis (PCA)}
PCA is one of the most common data compression algorithm in unsupervised learnings to reduce data dimensionality. This is particular useful when the underlying dataset too high in dimensions to learn efficiently. The result construct can be perceived as a projection of the target dataset $\mathcal{x}$ into $\mathcal{k}$ dimensions and is typically a linear transformation of given dataset $\mathcal{x}$.
\par 
One of the major benefit of PCA is the ability to transform underlying dataset into a representation with uncorrelated components (as shown in Figure~\ref{396105}), which result a lean representation of original dataset. Although correlation is one important relationship between data points within a dataset, to elucidate more complex feature dependencies require additional algorithms. 

\paragraph{$\mathcal{k}$-means Clustering}
In $\mathcal{k}$-means clustering, the training dataset is divided into $\mathcal{k}$ clusters that are close to each other.  $\mathcal{k}$-means clustering often utilise one-hot encoding to represent input dataset $\mathcal{x}$, where one-hot encoding adopts a string of only one high bit (i.e. one) and rest of low bit (i.e.zero) combination in representation~\cite{Harris:2012:DDC:2381028}. Therefore, $\mathcal{k}$-means clustering can be perceived as a representation with $\mathcal{k}$-dimensional one-hot vector $\Gamma$. For example, a data entry $\mathcal{x}_i \in$ cluster $\mathcal{i}$ can be presented by one-hot vector $\Gamma$ with $\gamma_i = 1$, the rest $\mathcal{k}-1$ entries equal to zero.
\par 
One-hot encoding is an extreme example of sparse representation and it offers great computational advantages, althrough due to extreme sparsity, it functions less efficiently in generalisation and support less network deepth~\cite{LeCun_2015} in comparison with distributed representation. In addition, despite the clustering property can be measured in average Euclidean distance within each cluster, clustering methods inherit ill posed problem, where there is hardly any quantitative criteria to measure the performance of the clustering in representing real dataset.  Consequently, distributed representation is currently preferred representation, particularlly in deep neural networks. 