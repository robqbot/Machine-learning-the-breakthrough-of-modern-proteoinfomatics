<div data-label="207704">Demonstration of a few activation functions&nbsp;commonly used in artificial neuron network: Sigmoid, RELU, Softmax and Sinusoid. a) Sigmoid.  As the archetypal sigmoid function, the output of the logistic function is always above zero, which in some training problems may cause zigzag effects and result slow in convergence. Hyperbolic tangent function corrects such defect by expanding output range between -1 and 1.&nbsp; Sigmoid functions, in general,&nbsp;have adverse squashing effects at either tail ends, which may cause inefficient learning at those regions.&nbsp; b) RELU. c) Softmax d)</div><div></div>