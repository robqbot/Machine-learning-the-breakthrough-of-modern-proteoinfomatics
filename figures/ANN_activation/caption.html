<div data-label="207704">Demonstration of a few activation functions&nbsp;commonly used in artificial neuron network: Sigmoid, RELU, Softmax and Sinusoid. As the archetypal sigmoid function, the output of the logistic function is always above zero, which in some training problems may cause zigzag effects and result slow in convergence. Hyperbolic tangent&nbsp;&nbsp;function correct such defect bu&nbsp;</div><div></div>