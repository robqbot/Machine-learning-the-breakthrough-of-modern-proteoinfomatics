\subsubsection{Other Regularization Algorithms}

Dropout~\cite{JMLR:v15:srivastava14a} also known as ensemble method, which was used to be another powerful regularization method, the main reason for its rise to popularity was due to its simplicity yet efficiency and is one of the less computationally expensive regularization option. Dropout is used on fully connected neuron network and has the benefit to capture the randomness in the data structure. Nevertheless, due to recent surge of modern convolutional architectures, the dropout almost goes out of favour due to its inefficiency in regularizing convolutional models. This arguably is due to generally low number of parameters in convolutional models and the strong correlation between layers; also the more modern adaptation moves away from fully connected model, which limited the application of dropout regularization. 
\par 
Global average pooling method introduced by~\citet{LinCY13} has introduced a final max pooling layer before apply activation function to predict the probability of each class. This method has become popular regularization method in imaginary recognition community with convolutional neural network. 
\par 
As an overfitted model is not only less reliable but also provide false information, developers have invested heavily in reducing the likely hold of model overfitting and model regularization is still a quick developing area, there are many regularization algorithms available like Fobenius parameter, matrix norm and many more, the decision of favouring a particular regularization algorithm is highly data and model dependent, sometimes computation intensity also pal